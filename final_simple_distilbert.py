# -*- coding: utf-8 -*-
"""Final_Simple_distilbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SRFjobZrKIB_rY3bHBzxMnz8u6jmNubG

[Simple Transformers](https://simpletransformers.ai/docs/binary-classification/)
"""

!pip install --upgrade transformers
!pip install simpletransformers
!pip install neattext

#Libraries for
import numpy as np
import pandas as pd
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from simpletransformers.classification import ClassificationModel, ClassificationArgs

#Libraries for Cleaning
import re

import nltk
nltk.download('all')

from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import neattext as nt
from neattext.pipeline import TextPipeline
from neattext.functions import  remove_emails, fix_contractions, remove_numbers,remove_emojis,remove_special_characters, remove_stopwords, remove_html_tags, remove_urls, remove_puncts,remove_custom_pattern

url = 'https://raw.githubusercontent.com/b3ccaMR/NLP-Disaster-Tweets/main/train.csv'
train_df = pd.read_csv(url)

train_df.head(5)

#filling in NA's for keyword and location
train_df['keyword'].fillna('no_keyword', inplace=True)
train_df['location'].fillna('no_location', inplace=True)

#graphing total counts of fake and real tweets
import seaborn as sns
import matplotlib.pyplot as plt

import matplotlib.patches as mpatches
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go


sns.countplot(x = train_df.target, data = train_df, palette = 'seismic_r')
labels = ["Non Disaster Tweets", "Disaster Tweets"]
plt.gca().set_xticklabels(labels)
plt.xlabel("Tweet Type")
plt.gca().set_title('Total Counts of Real and Fake Disaster Tweets')

#Number of fake and real tweets according to keyword, TOP 25 only
keyword_fake = train_df[train_df['target'] == 0].groupby('keyword').count().drop(columns=['target', 'id']).sort_values(by='text', ascending=False)[:25]
keyword_real = train_df[train_df['target'] == 1].groupby('keyword').count().drop(columns=['target', 'id']).sort_values(by='text', ascending=False)[:25]

sns.barplot(data=keyword_fake,y=keyword_real.index, x = 'text',color='red',) #number of fake tweets
sns.barplot(data=keyword_real,y=keyword_real.index, x = 'text',color='blue')#number of real tweets
plt.gca().set_xlabel('Number of tweets')
plt.gca().set_title('Number of tweets per keyword of Real and Fake Disaster Tweets')
red = mpatches.Patch(color='red', label='Non Disaster Tweet')
blue = mpatches.Patch(color='blue', label='Disaster Tweet')
plt.legend(loc=2, title='Tweet Types',bbox_to_anchor = (1,1), handles = [blue, red])

"""From above ouptut we can see the count of keywords which often appear in tweets related to both disaster tweets and non-diasater tweets."""

#Frequency distribution of tweets character lenght

real= train_df[train_df['target']==1]
fake = train_df[train_df['target']==0]

real_len = real['text'].str.len()
fake_len = fake['text'].str.len()

plt.hist(real_len, bins=50, alpha=1, label='Disaster Tweets', color='blue')
plt.hist(fake_len, bins=50, alpha=.8, label='Non-Disaster Tweets', color='red')

plt.xlabel('Number of characters')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet length')
plt.legend(title = "Tweet Types")
plt.show()

# Calculate the average word length for each set of tweets
real_avg_word_len = real["text"].str.split().apply(lambda x: sum(len(word) for word in x)/len(x)).mean()
fake_avg_word_len = fake["text"].str.split().apply(lambda x: sum(len(word) for word in x)/len(x)).mean()

sns.barplot(x = ["Disaster Tweets", "Non Disaster Tweets"],y = [real_avg_word_len,fake_avg_word_len], palette = 'seismic_r')
plt.title("Average Word Length in Real and Fake Disaster Tweets")
plt.xlabel("Tweet Type")
plt.ylabel("Average Word Length")
plt.show()

def clean_data(text):

    text = text.lower() # make it all lower case
    text = re.sub(r'\n', '', text) # take out any line breaks

    clean_pipeline = TextPipeline(steps=[remove_numbers, fix_contractions,
                                            remove_stopwords, remove_emails,remove_emojis,
                                            remove_html_tags, remove_urls, remove_special_characters])

    text = clean_pipeline.transform(text)

    lemmatizer = WordNetLemmatizer()
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])

    return text


# Clean text
train_df['clean_text'] = train_df['text'].apply(clean_data)
# Remove duplicates
train_df = train_df.drop_duplicates(subset='text', keep="first")
train_df[['text','clean_text']].head(5)

train_df.drop(['text','id', 'keyword', 'location'], axis=1, inplace=True)
#change target and clean_text to labels and text for simple transformer to work, could also reorder columns to put clean_text first
train_df.columns = ['labels', 'text']
x_train, x_val = train_test_split(train_df, test_size=0.2, stratify=train_df['labels'], random_state=42)
x_train.head(2)

"""[Training Tips](https://ufal.mff.cuni.cz/pbml/110/art-popel-bojar.pdf)
[Classifciation Args](https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model)

"""

model_args = ClassificationArgs(num_train_epochs=1, overwrite_output_dir=True, manual_seed=42, train_batch_size = 12)
model = ClassificationModel(model_type='distilbert', model_name='distilbert-base-uncased', use_cuda=False, num_labels=2, args=model_args)
#use_cude set to false because GPU

model.train_model(x_train)

result, model_outputs, wrong_preds = model.eval_model(x_val)

predictions = []
for x in model_outputs:
    predictions.append(np.argmax(x))
print('f1 score:', f1_score(x_val['labels'], predictions))

from sklearn.metrics import accuracy_score
predictions = []
for x in model_outputs:
    predictions.append(np.argmax(x))
print('accuracy score:', accuracy_score(x_val['labels'], predictions))

url = 'https://raw.githubusercontent.com/b3ccaMR/NLP-Disaster-Tweets/main/test.csv'
test_df = pd.read_csv(url)
print(test_df.head())
test_df['text'] = test_df['text'].apply(clean_data)

#not working?? It's only accepting a list of text and not a series
test_predictions, raw_outputs = model.predict(test_df['text'].to_list())

submission = pd.DataFrame({'id': test_df['id'], 'target': test_predictions })
#submission.to_csv('submission.csv', index=False)
submission.head()

from google.colab import files
from google.colab import drive
drive.mount('/content/drive')

submission.to_csv('/content/drive/MyDrive/notebooks/submission_transformer.csv', index=False)
print("Saved to Drive.")

